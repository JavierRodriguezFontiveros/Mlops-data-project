Proyecto Personal: Automatización de MLOps y Data Engineering

Introducción

Tras completar una formación intensiva en Data Science a través de un bootcamp, superar diversos retos en plataformas como Nuwe, y participar en competiciones de Kaggle en distintas vertientes del dato, he decidido emprender un proyecto personal que abarque de manera integral la automatización de procesos en MLOps y Data Engineering. Este desafío no solo pondrá a prueba mis conocimientos adquiridos, sino que también me permitirá enfrentarme a nuevos obstáculos y descubrir metodologías innovadoras en el campo de la ciencia de datos.

Objetivo

El objetivo principal de este proyecto es desarrollar una arquitectura escalable, automatizada y de bajo costo que permita la extracción, almacenamiento, procesamiento y modelado de datos, utilizando herramientas open-source y servicios gratuitos siempre que sea posible. Se busca predecir la evolución de los precios del aceite en Amazon mediante técnicas de series temporales.

Arquitectura y Tecnologías Utilizadas

1. Extracción de Datos (Web Scraping)

Herramientas: BeautifulSoup + requests (evita el alto consumo de Selenium).

Automatización: AWS Lambda con ejecuciones programadas o GitHub Actions.

Alternativa escalable: Scrapy con rotación de proxies.

2. Almacenamiento y Procesamiento

Base de datos: DynamoDB (NoSQL) para almacenamiento eficiente de producto, precio, categoría y fecha.

Alternativa para queries avanzadas: PostgreSQL (TimescaleDB) en Railway o AWS Free Tier.

3. API Backend

Framework: FastAPI para construir endpoints REST.

Endpoints clave:

/scraper: Ejecución del scraping manual o automática.

/data: Consulta de precios históricos.

/predict: Predicción de precios futuros.

/retrain: Reentrenamiento del modelo con nuevos datos.

Despliegue: Render o Railway con planes gratuitos.

4. Modelado de Series Temporales

Modelos utilizados:

Facebook Prophet (para predicciones rápidas y ajustables).

Alternativa: LSTM en TensorFlow si se requiere mayor precisión.

Entrenamiento: Google Colab Free o Hugging Face Spaces.

Seguimiento de modelos: DVC para versionado de datos y modelos.

5. Visualización de Datos y Dashboard

Tecnología: HTML + JS + Plotly Dash para flexibilidad y personalización.

Despliegue: Streamlit Cloud (alternativa gratuita) o Render.

6. Automatización y Orquestación

Scraping: AWS Lambda o cronjobs en GitHub Actions.

Pipelines de ML: Prefect (alternativa ligera a Airflow).

Contenedores: Docker para estandarizar ejecuciones.

Orquestación: Se evita Kubernetes inicialmente para reducir costos y complejidad.

Plan de Desarrollo y Seguimiento

Este proyecto evolucionará a medida que se vayan resolviendo los desafíos técnicos. Documentaré cada avance y obstáculo encontrado en este proceso, compartiendo actualizaciones en esta plataforma y en mi repositorio de GitHub.

Si deseas seguir el desarrollo en tiempo real, te invito a visitar mi repo: [GitHub Repository Link]

Este proyecto no solo consolidará mis habilidades en Data Engineering y MLOps, sino que también servirá como una referencia para otros entusiastas que quieran desarrollar soluciones escalables y automatizadas en el ámbito del Machine Learning.

