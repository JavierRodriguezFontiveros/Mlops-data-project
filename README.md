# Mlops-data-project


Proyecto Personal: AutomatizaciÃ³n de MLOps y Data Engineering

IntroducciÃ³n

Tras completar una formaciÃ³n intensiva en Data Science a travÃ©s de un bootcamp, superar diversos retos en plataformas como Nuwe, y participar en competiciones de Kaggle en distintas vertientes del dato, he decidido emprender un proyecto personal que abarque de manera integral la automatizaciÃ³n de procesos en MLOps y Data Engineering. Este desafÃ­o no solo pondrÃ¡ a prueba mis conocimientos adquiridos, sino que tambiÃ©n me permitirÃ¡ enfrentarme a nuevos obstÃ¡culos y descubrir metodologÃ­as innovadoras en el campo de la ciencia de datos.

Objetivo

El objetivo principal de este proyecto es desarrollar una arquitectura escalable, automatizada y de bajo costo que permita la extracciÃ³n, almacenamiento, procesamiento y modelado de datos, utilizando herramientas open-source y servicios gratuitos siempre que sea posible. Se busca predecir la evoluciÃ³n de los precios del aceite en Amazon mediante tÃ©cnicas de series temporales.

Arquitectura y TecnologÃ­as Utilizadas

1. ExtracciÃ³n de Datos (Web Scraping)

Herramientas: BeautifulSoup + requests (evita el alto consumo de Selenium).

AutomatizaciÃ³n: AWS Lambda con ejecuciones programadas o GitHub Actions.

Alternativa escalable: Scrapy con rotaciÃ³n de proxies.

2. Almacenamiento y Procesamiento

Base de datos: DynamoDB (NoSQL) para almacenamiento eficiente de producto, precio, categorÃ­a y fecha.

Alternativa para queries avanzadas: PostgreSQL (TimescaleDB) en Railway o AWS Free Tier.

3. API Backend

Framework: FastAPI para construir endpoints REST.

Endpoints clave:

/scraper: EjecuciÃ³n del scraping manual o automÃ¡tica.

/data: Consulta de precios histÃ³ricos.

/predict: PredicciÃ³n de precios futuros.

/retrain: Reentrenamiento del modelo con nuevos datos.

Despliegue: Render o Railway con planes gratuitos.

4. Modelado de Series Temporales

Modelos utilizados:

Facebook Prophet (para predicciones rÃ¡pidas y ajustables).

Alternativa: LSTM en TensorFlow si se requiere mayor precisiÃ³n.

Entrenamiento: Google Colab Free o Hugging Face Spaces.

Seguimiento de modelos: DVC para versionado de datos y modelos.

5. VisualizaciÃ³n de Datos y Dashboard

TecnologÃ­a: HTML + JS + Plotly Dash para flexibilidad y personalizaciÃ³n.

Despliegue: Streamlit Cloud (alternativa gratuita) o Render.

6. AutomatizaciÃ³n y OrquestaciÃ³n

Scraping: AWS Lambda o cronjobs en GitHub Actions.

Pipelines de ML: Prefect (alternativa ligera a Airflow).

Contenedores: Docker para estandarizar ejecuciones.

OrquestaciÃ³n: Se evita Kubernetes inicialmente para reducir costos y complejidad.

Plan de Desarrollo y Seguimiento

Este proyecto evolucionarÃ¡ a medida que se vayan resolviendo los desafÃ­os tÃ©cnicos. DocumentarÃ© cada avance y obstÃ¡culo encontrado en este proceso, compartiendo actualizaciones en esta plataforma y en mi repositorio de GitHub.

Si deseas seguir el desarrollo en tiempo real, te invito a visitar mi repo: [GitHub Repository Link]

Este proyecto no solo consolidarÃ¡ mis habilidades en Data Engineering y MLOps, sino que tambiÃ©n servirÃ¡ como una referencia para otros entusiastas que quieran desarrollar soluciones escalables y automatizadas en el Ã¡mbito del Machine Learning.

# ğŸ“‚ğŸ“‚ğŸ“‚

```bash
ğŸ“‚ mlops-data-project (Directorio raÃ­z del proyecto)
â”‚
â”œâ”€â”€ ğŸ“‚ data (Almacenamiento temporal de datos)
â”‚ â”œâ”€â”€ raw/ (Datos crudos extraÃ­dos del scraping)
â”‚ â”œâ”€â”€ processed/ (Datos limpios y listos para ML)
â”‚ â”œâ”€â”€ models/ (Modelos entrenados y versiones anteriores)
â”‚ â”œâ”€â”€ datasets/ (Datos histÃ³ricos para anÃ¡lisis y predicciÃ³n)
â”‚
â”œâ”€â”€ ğŸ“‚ scraper (CÃ³digo de web scraping con BeautifulSoup)
â”‚ â”œâ”€â”€ scraper.py (Script principal para extraer datos)
â”‚ â”œâ”€â”€ scraper_utils.py (Funciones auxiliares, headers, proxies...)
â”‚ â”œâ”€â”€ requirements.txt (Dependencias de scraping: BeautifulSoup, requests...)
â”‚
â”œâ”€â”€ ğŸ“‚ api (FastAPI para servir el backend)
â”‚ â”œâ”€â”€ main.py (Punto de entrada de la API, define endpoints)
â”‚ â”œâ”€â”€ routes.py (MÃ³dulo con rutas separadas para organizaciÃ³n)
â”‚ â”œâ”€â”€ models.py (DefiniciÃ³n de esquemas Pydantic y base de datos)
â”‚ â”œâ”€â”€ config.py (Configuraciones de la API y credenciales)
â”‚ â”œâ”€â”€ requirements.txt (Dependencias del backend: FastAPI, Uvicorn...)
â”‚
â”œâ”€â”€ ğŸ“‚ ml (Modelo de series temporales y entrenamiento)
â”‚ â”œâ”€â”€ train.py (CÃ³digo de entrenamiento del modelo Prophet/LSTM)
â”‚ â”œâ”€â”€ predict.py (Carga el modelo y genera predicciones)
â”‚ â”œâ”€â”€ evaluate.py (MÃ©tricas y evaluaciÃ³n del modelo)
â”‚ â”œâ”€â”€ ml_utils.py (Funciones auxiliares de ML)
â”‚ â”œâ”€â”€ requirements.txt (Dependencias: Prophet, TensorFlow, scikit-learn...)
â”‚
â”œâ”€â”€ ğŸ“‚ dashboard (Interfaz para visualizaciÃ³n de datos con Plotly Dash)
â”‚ â”œâ”€â”€ app.py (CÃ³digo principal del dashboard en Dash)
â”‚ â”œâ”€â”€ layout.py (DiseÃ±o de la interfaz y grÃ¡ficas)
â”‚ â”œâ”€â”€ callbacks.py (Funciones interactivas del dashboard)
â”‚ â”œâ”€â”€ requirements.txt (Dependencias: Dash, Plotly, Pandas...)
â”‚
â”œâ”€â”€ ğŸ“‚ automation (OrquestaciÃ³n y ejecuciÃ³n de tareas automatizadas)
â”‚ â”œâ”€â”€ pipeline.py (Pipeline de extracciÃ³n, almacenamiento y ML con Prefect/Airflow)
â”‚ â”œâ”€â”€ scheduler.py (ConfiguraciÃ³n de ejecuciÃ³n automÃ¡tica con AWS Lambda/GitHub Actions)
â”‚ â”œâ”€â”€ dockerfile (ConfiguraciÃ³n de Docker para la ejecuciÃ³n del proyecto)
â”‚ â”œâ”€â”€ k8s_deployment.yaml (Si en el futuro quieres usar Kubernetes)
â”‚
â”œâ”€â”€ ğŸ“‚ tests (Pruebas unitarias y de integraciÃ³n)
â”‚ â”œâ”€â”€ test_scraper.py (Tests para la extracciÃ³n de datos)
â”‚ â”œâ”€â”€ test_api.py (Tests para la API con pytest y FastAPI TestClient)
â”‚ â”œâ”€â”€ test_ml.py (Tests para validaciÃ³n del modelo de predicciÃ³n)
â”‚
â”œâ”€â”€ .gitignore (Archivos y carpetas a ignorar en GitHub, como modelos y credenciales)
â”œâ”€â”€ README.md (DescripciÃ³n del proyecto y documentaciÃ³n inicial)
â”œâ”€â”€ docker-compose.yml (Si necesitas levantar servicios como la base de datos localmente)
â”œâ”€â”€ config.yaml (Archivo de configuraciÃ³n global del proyecto)